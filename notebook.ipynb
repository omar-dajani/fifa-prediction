{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f25a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add the additional points to the score\n",
    "def add_points(ldm_score):\n",
    "    if isinstance(ldm_score, str) and len(ldm_score) > 1:\n",
    "        ldm = ldm_score[:-2]\n",
    "        points = ldm_score[-2:]\n",
    "        if points[0] == \"+\":\n",
    "            ldm = int(ldm) + int(points[1])\n",
    "        elif points[0] == \"-\":\n",
    "            ldm = int(ldm) - int(points[1])\n",
    "        else:\n",
    "            ldm = ldm_score\n",
    "        return ldm\n",
    "    else:\n",
    "\n",
    "        return ldm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f91c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher Parker\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (104) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Christopher Parker\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (81,82,83,84,85) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Christopher Parker\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (25,108) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(map(pd.read_csv,[\"FIFA Dataset game/players_15.csv\",\"FIFA Dataset game/players_16.csv\", \n",
    "                                \"FIFA Dataset game/players_17.csv\",\"FIFA Dataset game/players_18.csv\",\n",
    "                                \"FIFA Dataset game/players_19.csv\",\"FIFA Dataset game/players_20.csv\",\n",
    "                                \"FIFA Dataset game/players_21.csv\",\"FIFA Dataset game/players_22.csv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8ef0cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>potential</th>\n",
       "      <th>value_eur</th>\n",
       "      <th>wage_eur</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>release_clause_eur</th>\n",
       "      <th>pace</th>\n",
       "      <th>shooting</th>\n",
       "      <th>passing</th>\n",
       "      <th>dribbling</th>\n",
       "      <th>defending</th>\n",
       "      <th>physic</th>\n",
       "      <th>attacking_crossing</th>\n",
       "      <th>attacking_finishing</th>\n",
       "      <th>attacking_heading_accuracy</th>\n",
       "      <th>attacking_short_passing</th>\n",
       "      <th>attacking_volleys</th>\n",
       "      <th>skill_dribbling</th>\n",
       "      <th>skill_curve</th>\n",
       "      <th>skill_fk_accuracy</th>\n",
       "      <th>skill_long_passing</th>\n",
       "      <th>skill_ball_control</th>\n",
       "      <th>movement_acceleration</th>\n",
       "      <th>movement_sprint_speed</th>\n",
       "      <th>movement_agility</th>\n",
       "      <th>movement_reactions</th>\n",
       "      <th>movement_balance</th>\n",
       "      <th>power_shot_power</th>\n",
       "      <th>power_jumping</th>\n",
       "      <th>power_stamina</th>\n",
       "      <th>power_strength</th>\n",
       "      <th>power_long_shots</th>\n",
       "      <th>mentality_aggression</th>\n",
       "      <th>mentality_interceptions</th>\n",
       "      <th>mentality_positioning</th>\n",
       "      <th>mentality_vision</th>\n",
       "      <th>mentality_penalties</th>\n",
       "      <th>mentality_composure</th>\n",
       "      <th>defending_marking_awareness</th>\n",
       "      <th>defending_standing_tackle</th>\n",
       "      <th>defending_sliding_tackle</th>\n",
       "      <th>goalkeeping_diving</th>\n",
       "      <th>goalkeeping_handling</th>\n",
       "      <th>goalkeeping_kicking</th>\n",
       "      <th>goalkeeping_positioning</th>\n",
       "      <th>goalkeeping_reflexes</th>\n",
       "      <th>ls</th>\n",
       "      <th>st</th>\n",
       "      <th>rs</th>\n",
       "      <th>lw</th>\n",
       "      <th>lf</th>\n",
       "      <th>cf</th>\n",
       "      <th>rf</th>\n",
       "      <th>rw</th>\n",
       "      <th>lam</th>\n",
       "      <th>cam</th>\n",
       "      <th>ram</th>\n",
       "      <th>lm</th>\n",
       "      <th>lcm</th>\n",
       "      <th>cm</th>\n",
       "      <th>rcm</th>\n",
       "      <th>rm</th>\n",
       "      <th>lwb</th>\n",
       "      <th>ldm</th>\n",
       "      <th>cdm</th>\n",
       "      <th>rdm</th>\n",
       "      <th>rwb</th>\n",
       "      <th>lb</th>\n",
       "      <th>lcb</th>\n",
       "      <th>cb</th>\n",
       "      <th>rcb</th>\n",
       "      <th>rb</th>\n",
       "      <th>gk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.022619</td>\n",
       "      <td>3.741499</td>\n",
       "      <td>13.826243</td>\n",
       "      <td>25.856429</td>\n",
       "      <td>0.714945</td>\n",
       "      <td>0.846825</td>\n",
       "      <td>16.113629</td>\n",
       "      <td>1.985281</td>\n",
       "      <td>2.914196</td>\n",
       "      <td>2.384426</td>\n",
       "      <td>2.712586</td>\n",
       "      <td>-1.122990</td>\n",
       "      <td>1.548976</td>\n",
       "      <td>2.208526</td>\n",
       "      <td>2.712478</td>\n",
       "      <td>2.681756</td>\n",
       "      <td>2.095605</td>\n",
       "      <td>2.819151</td>\n",
       "      <td>2.462686</td>\n",
       "      <td>1.980433</td>\n",
       "      <td>1.995821</td>\n",
       "      <td>1.693180</td>\n",
       "      <td>2.999540</td>\n",
       "      <td>1.818968</td>\n",
       "      <td>2.022560</td>\n",
       "      <td>1.845367</td>\n",
       "      <td>3.822740</td>\n",
       "      <td>-0.294445</td>\n",
       "      <td>2.602711</td>\n",
       "      <td>2.495181</td>\n",
       "      <td>2.183612</td>\n",
       "      <td>1.131013</td>\n",
       "      <td>2.592453</td>\n",
       "      <td>0.249915</td>\n",
       "      <td>-1.143199</td>\n",
       "      <td>2.764741</td>\n",
       "      <td>2.301031</td>\n",
       "      <td>2.652752</td>\n",
       "      <td>3.380084</td>\n",
       "      <td>-1.596484</td>\n",
       "      <td>-1.111975</td>\n",
       "      <td>-1.418256</td>\n",
       "      <td>-1.059811</td>\n",
       "      <td>0.146230</td>\n",
       "      <td>1.350372</td>\n",
       "      <td>1.083513</td>\n",
       "      <td>0.155347</td>\n",
       "      <td>3.757186</td>\n",
       "      <td>3.757186</td>\n",
       "      <td>3.757186</td>\n",
       "      <td>3.520513</td>\n",
       "      <td>3.553520</td>\n",
       "      <td>3.553520</td>\n",
       "      <td>3.553520</td>\n",
       "      <td>3.520513</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.424255</td>\n",
       "      <td>2.918835</td>\n",
       "      <td>2.918835</td>\n",
       "      <td>2.918835</td>\n",
       "      <td>3.424255</td>\n",
       "      <td>1.171403</td>\n",
       "      <td>0.717066</td>\n",
       "      <td>0.717066</td>\n",
       "      <td>0.717066</td>\n",
       "      <td>1.171403</td>\n",
       "      <td>0.649751</td>\n",
       "      <td>-0.034715</td>\n",
       "      <td>-0.034715</td>\n",
       "      <td>-0.034715</td>\n",
       "      <td>0.649751</td>\n",
       "      <td>2.887322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.878363</td>\n",
       "      <td>3.577099</td>\n",
       "      <td>15.242155</td>\n",
       "      <td>25.856429</td>\n",
       "      <td>-1.600972</td>\n",
       "      <td>-0.348257</td>\n",
       "      <td>17.759350</td>\n",
       "      <td>1.896005</td>\n",
       "      <td>2.700239</td>\n",
       "      <td>2.767393</td>\n",
       "      <td>3.299387</td>\n",
       "      <td>-1.547855</td>\n",
       "      <td>-0.392733</td>\n",
       "      <td>1.637336</td>\n",
       "      <td>2.773721</td>\n",
       "      <td>1.222371</td>\n",
       "      <td>2.607506</td>\n",
       "      <td>2.615243</td>\n",
       "      <td>2.947447</td>\n",
       "      <td>2.512607</td>\n",
       "      <td>2.932248</td>\n",
       "      <td>2.501253</td>\n",
       "      <td>3.200075</td>\n",
       "      <td>2.075265</td>\n",
       "      <td>1.674004</td>\n",
       "      <td>1.926752</td>\n",
       "      <td>3.710252</td>\n",
       "      <td>2.340726</td>\n",
       "      <td>1.926847</td>\n",
       "      <td>0.183678</td>\n",
       "      <td>0.515840</td>\n",
       "      <td>-0.531891</td>\n",
       "      <td>2.339081</td>\n",
       "      <td>-0.798768</td>\n",
       "      <td>-1.517735</td>\n",
       "      <td>2.627145</td>\n",
       "      <td>2.690589</td>\n",
       "      <td>1.772310</td>\n",
       "      <td>3.477193</td>\n",
       "      <td>-2.102447</td>\n",
       "      <td>-1.270611</td>\n",
       "      <td>-1.259253</td>\n",
       "      <td>-1.365305</td>\n",
       "      <td>0.146230</td>\n",
       "      <td>1.350372</td>\n",
       "      <td>1.083513</td>\n",
       "      <td>-0.761556</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.418727</td>\n",
       "      <td>3.451718</td>\n",
       "      <td>3.451718</td>\n",
       "      <td>3.451718</td>\n",
       "      <td>3.418727</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.317569</td>\n",
       "      <td>3.424255</td>\n",
       "      <td>3.145195</td>\n",
       "      <td>3.145195</td>\n",
       "      <td>3.145195</td>\n",
       "      <td>3.424255</td>\n",
       "      <td>0.727490</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.727490</td>\n",
       "      <td>0.233144</td>\n",
       "      <td>-0.719277</td>\n",
       "      <td>-0.719277</td>\n",
       "      <td>-0.719277</td>\n",
       "      <td>0.233144</td>\n",
       "      <td>2.421197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.734107</td>\n",
       "      <td>3.412699</td>\n",
       "      <td>14.049808</td>\n",
       "      <td>22.423370</td>\n",
       "      <td>0.251761</td>\n",
       "      <td>1.743136</td>\n",
       "      <td>16.375256</td>\n",
       "      <td>1.271069</td>\n",
       "      <td>2.700239</td>\n",
       "      <td>2.097201</td>\n",
       "      <td>2.419186</td>\n",
       "      <td>-0.576735</td>\n",
       "      <td>1.651171</td>\n",
       "      <td>1.637336</td>\n",
       "      <td>2.712478</td>\n",
       "      <td>1.737448</td>\n",
       "      <td>2.095605</td>\n",
       "      <td>2.819151</td>\n",
       "      <td>2.058718</td>\n",
       "      <td>2.313041</td>\n",
       "      <td>2.530922</td>\n",
       "      <td>0.642685</td>\n",
       "      <td>2.799004</td>\n",
       "      <td>1.733535</td>\n",
       "      <td>0.802615</td>\n",
       "      <td>1.601212</td>\n",
       "      <td>3.485275</td>\n",
       "      <td>-0.541492</td>\n",
       "      <td>2.077039</td>\n",
       "      <td>0.269290</td>\n",
       "      <td>1.920280</td>\n",
       "      <td>1.131013</td>\n",
       "      <td>2.212395</td>\n",
       "      <td>1.298598</td>\n",
       "      <td>-0.501136</td>\n",
       "      <td>2.558348</td>\n",
       "      <td>2.223119</td>\n",
       "      <td>2.652752</td>\n",
       "      <td>2.214779</td>\n",
       "      <td>-1.146740</td>\n",
       "      <td>-0.371674</td>\n",
       "      <td>-0.623244</td>\n",
       "      <td>5.050062</td>\n",
       "      <td>4.479212</td>\n",
       "      <td>6.175620</td>\n",
       "      <td>6.948863</td>\n",
       "      <td>8.101838</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.538030</td>\n",
       "      <td>3.113371</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.113371</td>\n",
       "      <td>3.110867</td>\n",
       "      <td>3.110867</td>\n",
       "      <td>3.110867</td>\n",
       "      <td>2.987569</td>\n",
       "      <td>2.692476</td>\n",
       "      <td>2.692476</td>\n",
       "      <td>2.692476</td>\n",
       "      <td>2.987569</td>\n",
       "      <td>1.393359</td>\n",
       "      <td>1.013001</td>\n",
       "      <td>1.013001</td>\n",
       "      <td>1.013001</td>\n",
       "      <td>1.393359</td>\n",
       "      <td>0.962206</td>\n",
       "      <td>0.393137</td>\n",
       "      <td>0.393137</td>\n",
       "      <td>0.393137</td>\n",
       "      <td>0.962206</td>\n",
       "      <td>10.811448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.734107</td>\n",
       "      <td>3.741499</td>\n",
       "      <td>17.924936</td>\n",
       "      <td>12.124192</td>\n",
       "      <td>-0.829000</td>\n",
       "      <td>-0.945797</td>\n",
       "      <td>19.573863</td>\n",
       "      <td>2.163834</td>\n",
       "      <td>2.272325</td>\n",
       "      <td>2.097201</td>\n",
       "      <td>3.201587</td>\n",
       "      <td>-1.305075</td>\n",
       "      <td>-0.494928</td>\n",
       "      <td>1.494539</td>\n",
       "      <td>2.406262</td>\n",
       "      <td>0.449756</td>\n",
       "      <td>1.890845</td>\n",
       "      <td>2.479305</td>\n",
       "      <td>2.866654</td>\n",
       "      <td>1.980433</td>\n",
       "      <td>2.530922</td>\n",
       "      <td>1.531565</td>\n",
       "      <td>3.200075</td>\n",
       "      <td>2.246130</td>\n",
       "      <td>1.935421</td>\n",
       "      <td>2.415061</td>\n",
       "      <td>2.922834</td>\n",
       "      <td>1.270188</td>\n",
       "      <td>1.551367</td>\n",
       "      <td>-0.415600</td>\n",
       "      <td>0.954727</td>\n",
       "      <td>-1.007006</td>\n",
       "      <td>1.642309</td>\n",
       "      <td>-0.239471</td>\n",
       "      <td>-0.768662</td>\n",
       "      <td>2.420752</td>\n",
       "      <td>1.911473</td>\n",
       "      <td>2.332591</td>\n",
       "      <td>3.088758</td>\n",
       "      <td>-1.652702</td>\n",
       "      <td>-1.482126</td>\n",
       "      <td>-0.888248</td>\n",
       "      <td>-0.448824</td>\n",
       "      <td>-0.472768</td>\n",
       "      <td>1.350372</td>\n",
       "      <td>1.392216</td>\n",
       "      <td>0.155347</td>\n",
       "      <td>3.099719</td>\n",
       "      <td>3.099719</td>\n",
       "      <td>3.099719</td>\n",
       "      <td>3.316942</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.248114</td>\n",
       "      <td>3.316942</td>\n",
       "      <td>3.214218</td>\n",
       "      <td>3.214218</td>\n",
       "      <td>3.214218</td>\n",
       "      <td>3.205912</td>\n",
       "      <td>2.579296</td>\n",
       "      <td>2.579296</td>\n",
       "      <td>2.579296</td>\n",
       "      <td>3.205912</td>\n",
       "      <td>0.949447</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.421131</td>\n",
       "      <td>0.949447</td>\n",
       "      <td>0.441448</td>\n",
       "      <td>-0.633707</td>\n",
       "      <td>-0.633707</td>\n",
       "      <td>-0.633707</td>\n",
       "      <td>0.441448</td>\n",
       "      <td>2.887322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.589851</td>\n",
       "      <td>3.248299</td>\n",
       "      <td>13.304592</td>\n",
       "      <td>15.557251</td>\n",
       "      <td>0.714945</td>\n",
       "      <td>0.697440</td>\n",
       "      <td>12.400207</td>\n",
       "      <td>1.181793</td>\n",
       "      <td>2.557601</td>\n",
       "      <td>1.714234</td>\n",
       "      <td>2.321386</td>\n",
       "      <td>-0.819515</td>\n",
       "      <td>1.753366</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>2.528748</td>\n",
       "      <td>2.424217</td>\n",
       "      <td>2.095605</td>\n",
       "      <td>2.751182</td>\n",
       "      <td>1.977925</td>\n",
       "      <td>1.714346</td>\n",
       "      <td>2.530922</td>\n",
       "      <td>0.723492</td>\n",
       "      <td>2.598469</td>\n",
       "      <td>0.964644</td>\n",
       "      <td>1.325448</td>\n",
       "      <td>0.950133</td>\n",
       "      <td>3.260299</td>\n",
       "      <td>1.105490</td>\n",
       "      <td>2.152135</td>\n",
       "      <td>1.553458</td>\n",
       "      <td>1.042505</td>\n",
       "      <td>1.447757</td>\n",
       "      <td>2.022366</td>\n",
       "      <td>1.438423</td>\n",
       "      <td>-0.608146</td>\n",
       "      <td>2.489550</td>\n",
       "      <td>1.755649</td>\n",
       "      <td>2.332591</td>\n",
       "      <td>2.603214</td>\n",
       "      <td>-1.427830</td>\n",
       "      <td>-0.530310</td>\n",
       "      <td>-1.630259</td>\n",
       "      <td>1.384138</td>\n",
       "      <td>-1.401264</td>\n",
       "      <td>0.445638</td>\n",
       "      <td>-0.768703</td>\n",
       "      <td>-0.150288</td>\n",
       "      <td>3.428452</td>\n",
       "      <td>3.428452</td>\n",
       "      <td>3.428452</td>\n",
       "      <td>2.706230</td>\n",
       "      <td>3.044511</td>\n",
       "      <td>3.044511</td>\n",
       "      <td>3.044511</td>\n",
       "      <td>2.706230</td>\n",
       "      <td>2.697462</td>\n",
       "      <td>2.697462</td>\n",
       "      <td>2.697462</td>\n",
       "      <td>2.550884</td>\n",
       "      <td>2.352937</td>\n",
       "      <td>2.352937</td>\n",
       "      <td>2.352937</td>\n",
       "      <td>2.550884</td>\n",
       "      <td>0.505534</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.505534</td>\n",
       "      <td>0.233144</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.233144</td>\n",
       "      <td>1.955072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76719</th>\n",
       "      <td>-2.757417</td>\n",
       "      <td>-3.163304</td>\n",
       "      <td>-0.396966</td>\n",
       "      <td>-0.417917</td>\n",
       "      <td>-0.057028</td>\n",
       "      <td>-1.543338</td>\n",
       "      <td>-0.401476</td>\n",
       "      <td>-0.871566</td>\n",
       "      <td>-1.222306</td>\n",
       "      <td>-1.062276</td>\n",
       "      <td>-1.395016</td>\n",
       "      <td>-0.576735</td>\n",
       "      <td>-1.619075</td>\n",
       "      <td>-0.576025</td>\n",
       "      <td>-1.084594</td>\n",
       "      <td>-0.752090</td>\n",
       "      <td>-1.282940</td>\n",
       "      <td>-1.123064</td>\n",
       "      <td>-1.253816</td>\n",
       "      <td>-1.212611</td>\n",
       "      <td>-0.545910</td>\n",
       "      <td>-0.650232</td>\n",
       "      <td>-1.412232</td>\n",
       "      <td>-1.000301</td>\n",
       "      <td>-0.678748</td>\n",
       "      <td>-0.921718</td>\n",
       "      <td>-1.014255</td>\n",
       "      <td>0.281999</td>\n",
       "      <td>-1.001898</td>\n",
       "      <td>-0.329989</td>\n",
       "      <td>-1.415265</td>\n",
       "      <td>-1.561307</td>\n",
       "      <td>-1.334808</td>\n",
       "      <td>-0.519120</td>\n",
       "      <td>-0.447630</td>\n",
       "      <td>-1.156727</td>\n",
       "      <td>-0.971259</td>\n",
       "      <td>-0.788977</td>\n",
       "      <td>-2.252223</td>\n",
       "      <td>-0.696995</td>\n",
       "      <td>-0.477431</td>\n",
       "      <td>-0.093236</td>\n",
       "      <td>-1.365305</td>\n",
       "      <td>-0.163269</td>\n",
       "      <td>-1.665409</td>\n",
       "      <td>1.392216</td>\n",
       "      <td>0.766615</td>\n",
       "      <td>-1.502547</td>\n",
       "      <td>-1.502547</td>\n",
       "      <td>-1.502547</td>\n",
       "      <td>-1.365185</td>\n",
       "      <td>-1.434770</td>\n",
       "      <td>-1.434770</td>\n",
       "      <td>-1.434770</td>\n",
       "      <td>-1.365185</td>\n",
       "      <td>-1.333233</td>\n",
       "      <td>-1.333233</td>\n",
       "      <td>-1.333233</td>\n",
       "      <td>-1.379287</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.379287</td>\n",
       "      <td>-1.048159</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.048159</td>\n",
       "      <td>-0.912525</td>\n",
       "      <td>-0.804847</td>\n",
       "      <td>-0.804847</td>\n",
       "      <td>-0.804847</td>\n",
       "      <td>-0.912525</td>\n",
       "      <td>-0.375554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76720</th>\n",
       "      <td>-2.757417</td>\n",
       "      <td>-2.012504</td>\n",
       "      <td>-0.391005</td>\n",
       "      <td>-0.440804</td>\n",
       "      <td>-0.829000</td>\n",
       "      <td>-0.647027</td>\n",
       "      <td>-0.394809</td>\n",
       "      <td>-0.782290</td>\n",
       "      <td>-0.937030</td>\n",
       "      <td>-0.679309</td>\n",
       "      <td>-1.590616</td>\n",
       "      <td>-0.637430</td>\n",
       "      <td>-1.414685</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>-1.023351</td>\n",
       "      <td>-0.923783</td>\n",
       "      <td>-1.180559</td>\n",
       "      <td>-0.987125</td>\n",
       "      <td>-1.576990</td>\n",
       "      <td>0.117824</td>\n",
       "      <td>-1.014123</td>\n",
       "      <td>-0.488618</td>\n",
       "      <td>-2.114105</td>\n",
       "      <td>-0.658571</td>\n",
       "      <td>-0.853026</td>\n",
       "      <td>-0.189254</td>\n",
       "      <td>-1.464208</td>\n",
       "      <td>0.199650</td>\n",
       "      <td>-0.776610</td>\n",
       "      <td>-1.614157</td>\n",
       "      <td>-0.361935</td>\n",
       "      <td>-1.482121</td>\n",
       "      <td>-0.828065</td>\n",
       "      <td>-0.728856</td>\n",
       "      <td>-0.608146</td>\n",
       "      <td>-0.262357</td>\n",
       "      <td>-0.503789</td>\n",
       "      <td>-0.628896</td>\n",
       "      <td>-1.281136</td>\n",
       "      <td>-0.753213</td>\n",
       "      <td>-0.424552</td>\n",
       "      <td>-0.146237</td>\n",
       "      <td>0.162164</td>\n",
       "      <td>0.455728</td>\n",
       "      <td>-1.363830</td>\n",
       "      <td>-0.768703</td>\n",
       "      <td>-0.150288</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.160944</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.160944</td>\n",
       "      <td>-0.937181</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-0.937181</td>\n",
       "      <td>-0.912525</td>\n",
       "      <td>-0.975988</td>\n",
       "      <td>-0.975988</td>\n",
       "      <td>-0.975988</td>\n",
       "      <td>-0.912525</td>\n",
       "      <td>-0.841679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76721</th>\n",
       "      <td>-2.757417</td>\n",
       "      <td>-2.670104</td>\n",
       "      <td>-0.392495</td>\n",
       "      <td>-0.440804</td>\n",
       "      <td>-0.365816</td>\n",
       "      <td>-0.348257</td>\n",
       "      <td>-0.396328</td>\n",
       "      <td>-0.693013</td>\n",
       "      <td>-1.079668</td>\n",
       "      <td>-1.158018</td>\n",
       "      <td>-1.297216</td>\n",
       "      <td>-0.637430</td>\n",
       "      <td>-1.312490</td>\n",
       "      <td>-1.075817</td>\n",
       "      <td>-1.084594</td>\n",
       "      <td>-1.181321</td>\n",
       "      <td>-1.385320</td>\n",
       "      <td>-0.647279</td>\n",
       "      <td>-1.092229</td>\n",
       "      <td>-0.946524</td>\n",
       "      <td>-0.612797</td>\n",
       "      <td>-0.569425</td>\n",
       "      <td>-1.412232</td>\n",
       "      <td>-0.658571</td>\n",
       "      <td>-0.678748</td>\n",
       "      <td>-0.677564</td>\n",
       "      <td>-1.801672</td>\n",
       "      <td>-0.623841</td>\n",
       "      <td>-0.701514</td>\n",
       "      <td>-0.758045</td>\n",
       "      <td>-0.976378</td>\n",
       "      <td>-1.244564</td>\n",
       "      <td>-1.081436</td>\n",
       "      <td>-0.589032</td>\n",
       "      <td>-0.661651</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>-0.737524</td>\n",
       "      <td>-1.029097</td>\n",
       "      <td>-2.349332</td>\n",
       "      <td>-0.696995</td>\n",
       "      <td>-0.424552</td>\n",
       "      <td>-0.093236</td>\n",
       "      <td>-0.754317</td>\n",
       "      <td>-1.401264</td>\n",
       "      <td>-1.062252</td>\n",
       "      <td>-0.151297</td>\n",
       "      <td>-1.372825</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.365185</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.332968</td>\n",
       "      <td>-1.365185</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.270116</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.268812</td>\n",
       "      <td>-1.270116</td>\n",
       "      <td>-1.159137</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.058547</td>\n",
       "      <td>-1.159137</td>\n",
       "      <td>-1.016677</td>\n",
       "      <td>-0.890418</td>\n",
       "      <td>-0.890418</td>\n",
       "      <td>-0.890418</td>\n",
       "      <td>-1.016677</td>\n",
       "      <td>-1.773929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76722</th>\n",
       "      <td>-2.757417</td>\n",
       "      <td>-1.848104</td>\n",
       "      <td>-0.391005</td>\n",
       "      <td>-0.440804</td>\n",
       "      <td>-1.137789</td>\n",
       "      <td>-1.244568</td>\n",
       "      <td>-0.390927</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>-0.437797</td>\n",
       "      <td>-2.019693</td>\n",
       "      <td>-1.395016</td>\n",
       "      <td>-2.215499</td>\n",
       "      <td>-2.334442</td>\n",
       "      <td>-1.789804</td>\n",
       "      <td>-0.043462</td>\n",
       "      <td>-1.438859</td>\n",
       "      <td>-2.511501</td>\n",
       "      <td>-0.851187</td>\n",
       "      <td>-1.496197</td>\n",
       "      <td>-1.013046</td>\n",
       "      <td>-0.813460</td>\n",
       "      <td>-1.862342</td>\n",
       "      <td>-1.813303</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>-0.068775</td>\n",
       "      <td>0.461824</td>\n",
       "      <td>-1.576696</td>\n",
       "      <td>0.529046</td>\n",
       "      <td>-0.851706</td>\n",
       "      <td>-1.357323</td>\n",
       "      <td>-1.503042</td>\n",
       "      <td>-2.036423</td>\n",
       "      <td>-0.638036</td>\n",
       "      <td>-1.777540</td>\n",
       "      <td>-1.945777</td>\n",
       "      <td>-0.537548</td>\n",
       "      <td>-1.204993</td>\n",
       "      <td>-0.228695</td>\n",
       "      <td>-1.281136</td>\n",
       "      <td>-2.271102</td>\n",
       "      <td>-2.010912</td>\n",
       "      <td>-2.054265</td>\n",
       "      <td>-1.059811</td>\n",
       "      <td>-0.163269</td>\n",
       "      <td>-1.062252</td>\n",
       "      <td>1.083513</td>\n",
       "      <td>1.377884</td>\n",
       "      <td>-1.173813</td>\n",
       "      <td>-1.173813</td>\n",
       "      <td>-1.173813</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.436584</td>\n",
       "      <td>-1.436584</td>\n",
       "      <td>-1.436584</td>\n",
       "      <td>-1.597630</td>\n",
       "      <td>-2.174249</td>\n",
       "      <td>-2.174249</td>\n",
       "      <td>-2.174249</td>\n",
       "      <td>-1.597630</td>\n",
       "      <td>-2.712830</td>\n",
       "      <td>-2.735514</td>\n",
       "      <td>-2.735514</td>\n",
       "      <td>-2.735514</td>\n",
       "      <td>-2.712830</td>\n",
       "      <td>-2.578953</td>\n",
       "      <td>-2.516253</td>\n",
       "      <td>-2.516253</td>\n",
       "      <td>-2.516253</td>\n",
       "      <td>-2.578953</td>\n",
       "      <td>-0.375554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76723</th>\n",
       "      <td>-2.757417</td>\n",
       "      <td>-1.848104</td>\n",
       "      <td>-0.391005</td>\n",
       "      <td>-0.440804</td>\n",
       "      <td>-2.064155</td>\n",
       "      <td>-1.991494</td>\n",
       "      <td>-0.392784</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>-1.008349</td>\n",
       "      <td>-1.158018</td>\n",
       "      <td>-1.395016</td>\n",
       "      <td>-0.940905</td>\n",
       "      <td>-1.721270</td>\n",
       "      <td>-1.075817</td>\n",
       "      <td>-1.084594</td>\n",
       "      <td>-0.923783</td>\n",
       "      <td>-1.282940</td>\n",
       "      <td>-0.647279</td>\n",
       "      <td>-1.011435</td>\n",
       "      <td>-0.880002</td>\n",
       "      <td>-0.947236</td>\n",
       "      <td>-0.569425</td>\n",
       "      <td>-2.515175</td>\n",
       "      <td>0.195753</td>\n",
       "      <td>-0.068775</td>\n",
       "      <td>-0.107870</td>\n",
       "      <td>-0.901767</td>\n",
       "      <td>1.517235</td>\n",
       "      <td>-0.701514</td>\n",
       "      <td>-0.329989</td>\n",
       "      <td>-0.888600</td>\n",
       "      <td>-2.036423</td>\n",
       "      <td>-0.954751</td>\n",
       "      <td>-0.309383</td>\n",
       "      <td>-0.447630</td>\n",
       "      <td>-0.537548</td>\n",
       "      <td>-0.737524</td>\n",
       "      <td>-1.349258</td>\n",
       "      <td>-2.349332</td>\n",
       "      <td>-0.978085</td>\n",
       "      <td>-1.111975</td>\n",
       "      <td>-0.676245</td>\n",
       "      <td>-0.754317</td>\n",
       "      <td>0.765227</td>\n",
       "      <td>0.747216</td>\n",
       "      <td>-0.460000</td>\n",
       "      <td>1.072250</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.392969</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.231166</td>\n",
       "      <td>-1.263399</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.229882</td>\n",
       "      <td>-1.270116</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.381991</td>\n",
       "      <td>-1.270116</td>\n",
       "      <td>-1.381093</td>\n",
       "      <td>-1.255837</td>\n",
       "      <td>-1.255837</td>\n",
       "      <td>-1.255837</td>\n",
       "      <td>-1.381093</td>\n",
       "      <td>-1.224980</td>\n",
       "      <td>-1.232699</td>\n",
       "      <td>-1.232699</td>\n",
       "      <td>-1.232699</td>\n",
       "      <td>-1.224980</td>\n",
       "      <td>0.090571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76724 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall  potential  value_eur   wage_eur  height_cm  weight_kg  \\\n",
       "0      4.022619   3.741499  13.826243  25.856429   0.714945   0.846825   \n",
       "1      3.878363   3.577099  15.242155  25.856429  -1.600972  -0.348257   \n",
       "2      3.734107   3.412699  14.049808  22.423370   0.251761   1.743136   \n",
       "3      3.734107   3.741499  17.924936  12.124192  -0.829000  -0.945797   \n",
       "4      3.589851   3.248299  13.304592  15.557251   0.714945   0.697440   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "76719 -2.757417  -3.163304  -0.396966  -0.417917  -0.057028  -1.543338   \n",
       "76720 -2.757417  -2.012504  -0.391005  -0.440804  -0.829000  -0.647027   \n",
       "76721 -2.757417  -2.670104  -0.392495  -0.440804  -0.365816  -0.348257   \n",
       "76722 -2.757417  -1.848104  -0.391005  -0.440804  -1.137789  -1.244568   \n",
       "76723 -2.757417  -1.848104  -0.391005  -0.440804  -2.064155  -1.991494   \n",
       "\n",
       "       release_clause_eur      pace  shooting   passing  dribbling  defending  \\\n",
       "0               16.113629  1.985281  2.914196  2.384426   2.712586  -1.122990   \n",
       "1               17.759350  1.896005  2.700239  2.767393   3.299387  -1.547855   \n",
       "2               16.375256  1.271069  2.700239  2.097201   2.419186  -0.576735   \n",
       "3               19.573863  2.163834  2.272325  2.097201   3.201587  -1.305075   \n",
       "4               12.400207  1.181793  2.557601  1.714234   2.321386  -0.819515   \n",
       "...                   ...       ...       ...       ...        ...        ...   \n",
       "76719           -0.401476 -0.871566 -1.222306 -1.062276  -1.395016  -0.576735   \n",
       "76720           -0.394809 -0.782290 -0.937030 -0.679309  -1.590616  -0.637430   \n",
       "76721           -0.396328 -0.693013 -1.079668 -1.158018  -1.297216  -0.637430   \n",
       "76722           -0.390927  0.021199 -0.437797 -2.019693  -1.395016  -2.215499   \n",
       "76723           -0.392784  0.021199 -1.008349 -1.158018  -1.395016  -0.940905   \n",
       "\n",
       "         physic  attacking_crossing  attacking_finishing  \\\n",
       "0      1.548976            2.208526             2.712478   \n",
       "1     -0.392733            1.637336             2.773721   \n",
       "2      1.651171            1.637336             2.712478   \n",
       "3     -0.494928            1.494539             2.406262   \n",
       "4      1.753366            0.566355             2.528748   \n",
       "...         ...                 ...                  ...   \n",
       "76719 -1.619075           -0.576025            -1.084594   \n",
       "76720 -1.414685           -0.004835            -1.023351   \n",
       "76721 -1.312490           -1.075817            -1.084594   \n",
       "76722 -2.334442           -1.789804            -0.043462   \n",
       "76723 -1.721270           -1.075817            -1.084594   \n",
       "\n",
       "       attacking_heading_accuracy  attacking_short_passing  attacking_volleys  \\\n",
       "0                        2.681756                 2.095605           2.819151   \n",
       "1                        1.222371                 2.607506           2.615243   \n",
       "2                        1.737448                 2.095605           2.819151   \n",
       "3                        0.449756                 1.890845           2.479305   \n",
       "4                        2.424217                 2.095605           2.751182   \n",
       "...                           ...                      ...                ...   \n",
       "76719                   -0.752090                -1.282940          -1.123064   \n",
       "76720                   -0.923783                -1.180559          -0.987125   \n",
       "76721                   -1.181321                -1.385320          -0.647279   \n",
       "76722                   -1.438859                -2.511501          -0.851187   \n",
       "76723                   -0.923783                -1.282940          -0.647279   \n",
       "\n",
       "       skill_dribbling  skill_curve  skill_fk_accuracy  skill_long_passing  \\\n",
       "0             2.462686     1.980433           1.995821            1.693180   \n",
       "1             2.947447     2.512607           2.932248            2.501253   \n",
       "2             2.058718     2.313041           2.530922            0.642685   \n",
       "3             2.866654     1.980433           2.530922            1.531565   \n",
       "4             1.977925     1.714346           2.530922            0.723492   \n",
       "...                ...          ...                ...                 ...   \n",
       "76719        -1.253816    -1.212611          -0.545910           -0.650232   \n",
       "76720        -1.576990     0.117824          -1.014123           -0.488618   \n",
       "76721        -1.092229    -0.946524          -0.612797           -0.569425   \n",
       "76722        -1.496197    -1.013046          -0.813460           -1.862342   \n",
       "76723        -1.011435    -0.880002          -0.947236           -0.569425   \n",
       "\n",
       "       skill_ball_control  movement_acceleration  movement_sprint_speed  \\\n",
       "0                2.999540               1.818968               2.022560   \n",
       "1                3.200075               2.075265               1.674004   \n",
       "2                2.799004               1.733535               0.802615   \n",
       "3                3.200075               2.246130               1.935421   \n",
       "4                2.598469               0.964644               1.325448   \n",
       "...                   ...                    ...                    ...   \n",
       "76719           -1.412232              -1.000301              -0.678748   \n",
       "76720           -2.114105              -0.658571              -0.853026   \n",
       "76721           -1.412232              -0.658571              -0.678748   \n",
       "76722           -1.813303               0.110320              -0.068775   \n",
       "76723           -2.515175               0.195753              -0.068775   \n",
       "\n",
       "       movement_agility  movement_reactions  movement_balance  \\\n",
       "0              1.845367            3.822740         -0.294445   \n",
       "1              1.926752            3.710252          2.340726   \n",
       "2              1.601212            3.485275         -0.541492   \n",
       "3              2.415061            2.922834          1.270188   \n",
       "4              0.950133            3.260299          1.105490   \n",
       "...                 ...                 ...               ...   \n",
       "76719         -0.921718           -1.014255          0.281999   \n",
       "76720         -0.189254           -1.464208          0.199650   \n",
       "76721         -0.677564           -1.801672         -0.623841   \n",
       "76722          0.461824           -1.576696          0.529046   \n",
       "76723         -0.107870           -0.901767          1.517235   \n",
       "\n",
       "       power_shot_power  power_jumping  power_stamina  power_strength  \\\n",
       "0              2.602711       2.495181       2.183612        1.131013   \n",
       "1              1.926847       0.183678       0.515840       -0.531891   \n",
       "2              2.077039       0.269290       1.920280        1.131013   \n",
       "3              1.551367      -0.415600       0.954727       -1.007006   \n",
       "4              2.152135       1.553458       1.042505        1.447757   \n",
       "...                 ...            ...            ...             ...   \n",
       "76719         -1.001898      -0.329989      -1.415265       -1.561307   \n",
       "76720         -0.776610      -1.614157      -0.361935       -1.482121   \n",
       "76721         -0.701514      -0.758045      -0.976378       -1.244564   \n",
       "76722         -0.851706      -1.357323      -1.503042       -2.036423   \n",
       "76723         -0.701514      -0.329989      -0.888600       -2.036423   \n",
       "\n",
       "       power_long_shots  mentality_aggression  mentality_interceptions  \\\n",
       "0              2.592453              0.249915                -1.143199   \n",
       "1              2.339081             -0.798768                -1.517735   \n",
       "2              2.212395              1.298598                -0.501136   \n",
       "3              1.642309             -0.239471                -0.768662   \n",
       "4              2.022366              1.438423                -0.608146   \n",
       "...                 ...                   ...                      ...   \n",
       "76719         -1.334808             -0.519120                -0.447630   \n",
       "76720         -0.828065             -0.728856                -0.608146   \n",
       "76721         -1.081436             -0.589032                -0.661651   \n",
       "76722         -0.638036             -1.777540                -1.945777   \n",
       "76723         -0.954751             -0.309383                -0.447630   \n",
       "\n",
       "       mentality_positioning  mentality_vision  mentality_penalties  \\\n",
       "0                   2.764741          2.301031             2.652752   \n",
       "1                   2.627145          2.690589             1.772310   \n",
       "2                   2.558348          2.223119             2.652752   \n",
       "3                   2.420752          1.911473             2.332591   \n",
       "4                   2.489550          1.755649             2.332591   \n",
       "...                      ...               ...                  ...   \n",
       "76719              -1.156727         -0.971259            -0.788977   \n",
       "76720              -0.262357         -0.503789            -0.628896   \n",
       "76721              -0.675143         -0.737524            -1.029097   \n",
       "76722              -0.537548         -1.204993            -0.228695   \n",
       "76723              -0.537548         -0.737524            -1.349258   \n",
       "\n",
       "       mentality_composure  defending_marking_awareness  \\\n",
       "0                 3.380084                    -1.596484   \n",
       "1                 3.477193                    -2.102447   \n",
       "2                 2.214779                    -1.146740   \n",
       "3                 3.088758                    -1.652702   \n",
       "4                 2.603214                    -1.427830   \n",
       "...                    ...                          ...   \n",
       "76719            -2.252223                    -0.696995   \n",
       "76720            -1.281136                    -0.753213   \n",
       "76721            -2.349332                    -0.696995   \n",
       "76722            -1.281136                    -2.271102   \n",
       "76723            -2.349332                    -0.978085   \n",
       "\n",
       "       defending_standing_tackle  defending_sliding_tackle  \\\n",
       "0                      -1.111975                 -1.418256   \n",
       "1                      -1.270611                 -1.259253   \n",
       "2                      -0.371674                 -0.623244   \n",
       "3                      -1.482126                 -0.888248   \n",
       "4                      -0.530310                 -1.630259   \n",
       "...                          ...                       ...   \n",
       "76719                  -0.477431                 -0.093236   \n",
       "76720                  -0.424552                 -0.146237   \n",
       "76721                  -0.424552                 -0.093236   \n",
       "76722                  -2.010912                 -2.054265   \n",
       "76723                  -1.111975                 -0.676245   \n",
       "\n",
       "       goalkeeping_diving  goalkeeping_handling  goalkeeping_kicking  \\\n",
       "0               -1.059811              0.146230             1.350372   \n",
       "1               -1.365305              0.146230             1.350372   \n",
       "2                5.050062              4.479212             6.175620   \n",
       "3               -0.448824             -0.472768             1.350372   \n",
       "4                1.384138             -1.401264             0.445638   \n",
       "...                   ...                   ...                  ...   \n",
       "76719           -1.365305             -0.163269            -1.665409   \n",
       "76720            0.162164              0.455728            -1.363830   \n",
       "76721           -0.754317             -1.401264            -1.062252   \n",
       "76722           -1.059811             -0.163269            -1.062252   \n",
       "76723           -0.754317              0.765227             0.747216   \n",
       "\n",
       "       goalkeeping_positioning  goalkeeping_reflexes        ls        st  \\\n",
       "0                     1.083513              0.155347  3.757186  3.757186   \n",
       "1                     1.083513             -0.761556  3.538030  3.538030   \n",
       "2                     6.948863              8.101838  3.538030  3.538030   \n",
       "3                     1.392216              0.155347  3.099719  3.099719   \n",
       "4                    -0.768703             -0.150288  3.428452  3.428452   \n",
       "...                        ...                   ...       ...       ...   \n",
       "76719                 1.392216              0.766615 -1.502547 -1.502547   \n",
       "76720                -0.768703             -0.150288 -1.392969 -1.392969   \n",
       "76721                -0.151297             -1.372825 -1.392969 -1.392969   \n",
       "76722                 1.083513              1.377884 -1.173813 -1.173813   \n",
       "76723                -0.460000              1.072250 -1.392969 -1.392969   \n",
       "\n",
       "             rs        lw        lf        cf        rf        rw       lam  \\\n",
       "0      3.757186  3.520513  3.553520  3.553520  3.553520  3.520513  3.317569   \n",
       "1      3.538030  3.418727  3.451718  3.451718  3.451718  3.418727  3.317569   \n",
       "2      3.538030  3.113371  3.248114  3.248114  3.248114  3.113371  3.110867   \n",
       "3      3.099719  3.316942  3.248114  3.248114  3.248114  3.316942  3.214218   \n",
       "4      3.428452  2.706230  3.044511  3.044511  3.044511  2.706230  2.697462   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "76719 -1.502547 -1.365185 -1.434770 -1.434770 -1.434770 -1.365185 -1.333233   \n",
       "76720 -1.392969 -1.263399 -1.332968 -1.332968 -1.332968 -1.263399 -1.229882   \n",
       "76721 -1.392969 -1.365185 -1.332968 -1.332968 -1.332968 -1.365185 -1.229882   \n",
       "76722 -1.173813 -1.263399 -1.231166 -1.231166 -1.231166 -1.263399 -1.436584   \n",
       "76723 -1.392969 -1.263399 -1.231166 -1.231166 -1.231166 -1.263399 -1.229882   \n",
       "\n",
       "            cam       ram        lm       lcm        cm       rcm        rm  \\\n",
       "0      3.317569  3.317569  3.424255  2.918835  2.918835  2.918835  3.424255   \n",
       "1      3.317569  3.317569  3.424255  3.145195  3.145195  3.145195  3.424255   \n",
       "2      3.110867  3.110867  2.987569  2.692476  2.692476  2.692476  2.987569   \n",
       "3      3.214218  3.214218  3.205912  2.579296  2.579296  2.579296  3.205912   \n",
       "4      2.697462  2.697462  2.550884  2.352937  2.352937  2.352937  2.550884   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "76719 -1.333233 -1.333233 -1.379287 -1.381991 -1.381991 -1.381991 -1.379287   \n",
       "76720 -1.229882 -1.229882 -1.160944 -1.268812 -1.268812 -1.268812 -1.160944   \n",
       "76721 -1.229882 -1.229882 -1.270116 -1.268812 -1.268812 -1.268812 -1.270116   \n",
       "76722 -1.436584 -1.436584 -1.597630 -2.174249 -2.174249 -2.174249 -1.597630   \n",
       "76723 -1.229882 -1.229882 -1.270116 -1.381991 -1.381991 -1.381991 -1.270116   \n",
       "\n",
       "            lwb       ldm       cdm       rdm       rwb        lb       lcb  \\\n",
       "0      1.171403  0.717066  0.717066  0.717066  1.171403  0.649751 -0.034715   \n",
       "1      0.727490  0.421131  0.421131  0.421131  0.727490  0.233144 -0.719277   \n",
       "2      1.393359  1.013001  1.013001  1.013001  1.393359  0.962206  0.393137   \n",
       "3      0.949447  0.421131  0.421131  0.421131  0.949447  0.441448 -0.633707   \n",
       "4      0.505534  0.618421  0.618421  0.618421  0.505534  0.233144  0.221996   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "76719 -1.048159 -1.058547 -1.058547 -1.058547 -1.048159 -0.912525 -0.804847   \n",
       "76720 -0.937181 -1.058547 -1.058547 -1.058547 -0.937181 -0.912525 -0.975988   \n",
       "76721 -1.159137 -1.058547 -1.058547 -1.058547 -1.159137 -1.016677 -0.890418   \n",
       "76722 -2.712830 -2.735514 -2.735514 -2.735514 -2.712830 -2.578953 -2.516253   \n",
       "76723 -1.381093 -1.255837 -1.255837 -1.255837 -1.381093 -1.224980 -1.232699   \n",
       "\n",
       "             cb       rcb        rb         gk  \n",
       "0     -0.034715 -0.034715  0.649751   2.887322  \n",
       "1     -0.719277 -0.719277  0.233144   2.421197  \n",
       "2      0.393137  0.393137  0.962206  10.811448  \n",
       "3     -0.633707 -0.633707  0.441448   2.887322  \n",
       "4      0.221996  0.221996  0.233144   1.955072  \n",
       "...         ...       ...       ...        ...  \n",
       "76719 -0.804847 -0.804847 -0.912525  -0.375554  \n",
       "76720 -0.975988 -0.975988 -0.912525  -0.841679  \n",
       "76721 -0.890418 -0.890418 -1.016677  -1.773929  \n",
       "76722 -2.516253 -2.516253 -2.578953  -0.375554  \n",
       "76723 -1.232699 -1.232699 -1.224980   0.090571  \n",
       "\n",
       "[76724 rows x 74 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from tensorflow import keras\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# load the dataset\n",
    "# drop irrelevant columns\n",
    "df_drop = ['sofifa_id', 'nation_flag_url', 'nation_logo_url', 'club_flag_url', 'club_logo_url',\n",
    "           'player_face_url', 'goalkeeping_speed', 'player_traits', 'player_tags', \n",
    "           'body_type', 'real_face', 'work_rate', 'preferred_foot', 'nation_jersey_number',\n",
    "           'nation_position', 'nation_team_id', 'nationality_name', 'club_contract_valid_until',\n",
    "           'club_joined', 'club_loaned_from', 'club_jersey_number', 'club_position', 'league_level',\n",
    "           'league_name', 'club_name', 'player_positions', 'long_name', 'short_name', 'player_url', 'nationality_id', 'club_team_id', 'dob', 'weak_foot', 'skill_moves', 'international_reputation']\n",
    "# columns that are in the format of score +/- x that need to be converted to score + x\n",
    "cols_adjusted = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm',\n",
    "                 'cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk']\n",
    "\n",
    "X = df.drop(df_drop, axis=1)\n",
    "\n",
    "X = X.dropna(how='any')\n",
    "y = X['age']\n",
    "X = X.drop('age', axis=1)\n",
    "# adjust columns with a format of score +/- x\n",
    "for col_adjust in cols_adjusted:\n",
    "    X[col_adjust] = X[col_adjust].apply(add_points)\n",
    "    \n",
    "# X = X.astype(float)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "23c89fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1535/1535 [==============================] - 4s 2ms/step - loss: 30.6939 - val_loss: 4.2688\n",
      "Epoch 2/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 3.6888 - val_loss: 3.1336\n",
      "Epoch 3/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.9584 - val_loss: 2.6710\n",
      "Epoch 4/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.6408 - val_loss: 2.5417\n",
      "Epoch 5/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 2.4888 - val_loss: 2.2974\n",
      "Epoch 6/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 2.3823 - val_loss: 2.2521\n",
      "Epoch 7/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 2.3240 - val_loss: 2.3348\n",
      "Epoch 8/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.2425 - val_loss: 2.2631\n",
      "Epoch 9/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.1799 - val_loss: 2.1090\n",
      "Epoch 10/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.1197 - val_loss: 2.0025\n",
      "Epoch 11/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 2.0774 - val_loss: 2.2909\n",
      "Epoch 12/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 2.0175 - val_loss: 1.8993\n",
      "Epoch 13/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.9753 - val_loss: 1.8904\n",
      "Epoch 14/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.9262 - val_loss: 1.9069\n",
      "Epoch 15/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.8763 - val_loss: 2.0489\n",
      "Epoch 16/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.8349 - val_loss: 2.3104\n",
      "Epoch 17/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.8243 - val_loss: 1.8615\n",
      "Epoch 18/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.7802 - val_loss: 1.7895\n",
      "Epoch 19/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.7316 - val_loss: 1.7478\n",
      "Epoch 20/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.7096 - val_loss: 1.6691\n",
      "Epoch 21/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.7000 - val_loss: 1.6537\n",
      "Epoch 22/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.6923 - val_loss: 1.7193\n",
      "Epoch 23/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.6869 - val_loss: 1.8256\n",
      "Epoch 24/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.6420 - val_loss: 1.6970\n",
      "Epoch 25/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.6289 - val_loss: 1.7023\n",
      "Epoch 26/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.6203 - val_loss: 1.9955\n",
      "Epoch 27/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.6073 - val_loss: 1.6058\n",
      "Epoch 28/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5891 - val_loss: 1.7433\n",
      "Epoch 29/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5761 - val_loss: 1.6720\n",
      "Epoch 30/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.5740 - val_loss: 1.6977\n",
      "Epoch 31/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5760 - val_loss: 1.6690\n",
      "Epoch 32/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5415 - val_loss: 1.6696\n",
      "Epoch 33/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5364 - val_loss: 1.6663\n",
      "Epoch 34/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5165 - val_loss: 1.6145\n",
      "Epoch 35/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.5270 - val_loss: 1.5049\n",
      "Epoch 36/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.5127 - val_loss: 1.6363\n",
      "Epoch 37/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.4797 - val_loss: 2.0097\n",
      "Epoch 38/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4942 - val_loss: 1.6238\n",
      "Epoch 39/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4884 - val_loss: 1.5037\n",
      "Epoch 40/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4707 - val_loss: 1.6009\n",
      "Epoch 41/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.4651 - val_loss: 1.5182\n",
      "Epoch 42/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4541 - val_loss: 1.7039\n",
      "Epoch 43/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4570 - val_loss: 1.5135\n",
      "Epoch 44/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.4527 - val_loss: 1.6593\n",
      "Epoch 45/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4526 - val_loss: 1.6117\n",
      "Epoch 46/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4278 - val_loss: 1.5182\n",
      "Epoch 47/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4236 - val_loss: 1.6791\n",
      "Epoch 48/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4193 - val_loss: 1.5167\n",
      "Epoch 49/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4192 - val_loss: 1.4706\n",
      "Epoch 50/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4195 - val_loss: 1.5594\n",
      "Epoch 51/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.4135 - val_loss: 1.4630\n",
      "Epoch 52/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3885 - val_loss: 1.5850\n",
      "Epoch 53/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.4062 - val_loss: 1.4970\n",
      "Epoch 54/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3931 - val_loss: 1.4611\n",
      "Epoch 55/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3795 - val_loss: 1.6187\n",
      "Epoch 56/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.3786 - val_loss: 1.5646\n",
      "Epoch 57/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3774 - val_loss: 1.5080\n",
      "Epoch 58/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.3629 - val_loss: 1.4783\n",
      "Epoch 59/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3547 - val_loss: 1.5314\n",
      "Epoch 60/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3602 - val_loss: 1.4468\n",
      "Epoch 61/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3559 - val_loss: 1.4640\n",
      "Epoch 62/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3439 - val_loss: 1.5083\n",
      "Epoch 63/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3498 - val_loss: 1.4465\n",
      "Epoch 64/200\n",
      "1535/1535 [==============================] - 2s 1ms/step - loss: 1.3420 - val_loss: 1.4965\n",
      "Epoch 65/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3370 - val_loss: 1.4599\n",
      "Epoch 66/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.3225 - val_loss: 1.5119\n",
      "Epoch 67/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3273 - val_loss: 1.6341\n",
      "Epoch 68/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3138 - val_loss: 1.5308\n",
      "Epoch 69/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3283 - val_loss: 1.4163\n",
      "Epoch 70/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3032 - val_loss: 1.5002\n",
      "Epoch 71/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3063 - val_loss: 1.5345\n",
      "Epoch 72/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.3015 - val_loss: 1.4573\n",
      "Epoch 73/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2948 - val_loss: 1.6892\n",
      "Epoch 74/200\n",
      "1535/1535 [==============================] - 2s 1ms/step - loss: 1.3057 - val_loss: 1.3945\n",
      "Epoch 75/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2860 - val_loss: 1.4769\n",
      "Epoch 76/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2901 - val_loss: 1.4920\n",
      "Epoch 77/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2977 - val_loss: 1.4819\n",
      "Epoch 78/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2691 - val_loss: 1.4508\n",
      "Epoch 79/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2794 - val_loss: 1.4249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2844 - val_loss: 1.5621\n",
      "Epoch 81/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2688 - val_loss: 1.4558\n",
      "Epoch 82/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2731 - val_loss: 1.4271\n",
      "Epoch 83/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2817 - val_loss: 1.4626\n",
      "Epoch 84/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2599 - val_loss: 1.4166\n",
      "Epoch 85/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2568 - val_loss: 1.5446\n",
      "Epoch 86/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2512 - val_loss: 1.4743\n",
      "Epoch 87/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2523 - val_loss: 1.4126\n",
      "Epoch 88/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2427 - val_loss: 1.4643\n",
      "Epoch 89/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2508 - val_loss: 1.4531\n",
      "Epoch 90/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2450 - val_loss: 1.3757\n",
      "Epoch 91/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2333 - val_loss: 1.5001\n",
      "Epoch 92/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2310 - val_loss: 1.4541\n",
      "Epoch 93/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2297 - val_loss: 1.5581\n",
      "Epoch 94/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2393 - val_loss: 1.3802\n",
      "Epoch 95/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2294 - val_loss: 1.4435\n",
      "Epoch 96/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2339 - val_loss: 1.4808\n",
      "Epoch 97/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2226 - val_loss: 1.4997\n",
      "Epoch 98/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2247 - val_loss: 1.4628\n",
      "Epoch 99/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2126 - val_loss: 1.4924\n",
      "Epoch 100/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2248 - val_loss: 1.5318\n",
      "Epoch 101/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2073 - val_loss: 1.4052\n",
      "Epoch 102/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2072 - val_loss: 1.4353\n",
      "Epoch 103/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2133 - val_loss: 1.4312\n",
      "Epoch 104/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2084 - val_loss: 1.4266\n",
      "Epoch 105/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2175 - val_loss: 1.4175\n",
      "Epoch 106/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1977 - val_loss: 1.5522\n",
      "Epoch 107/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2053 - val_loss: 1.4576\n",
      "Epoch 108/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1820 - val_loss: 1.4015\n",
      "Epoch 109/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.2016 - val_loss: 1.3826\n",
      "Epoch 110/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.2003 - val_loss: 1.3897\n",
      "Epoch 111/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1917 - val_loss: 1.3844\n",
      "Epoch 112/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1930 - val_loss: 1.4505\n",
      "Epoch 113/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1872 - val_loss: 1.5356\n",
      "Epoch 114/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1795 - val_loss: 1.4717\n",
      "Epoch 115/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1870 - val_loss: 1.4060\n",
      "Epoch 116/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1711 - val_loss: 1.4660\n",
      "Epoch 117/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1726 - val_loss: 1.4461\n",
      "Epoch 118/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1941 - val_loss: 1.4015\n",
      "Epoch 119/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1735 - val_loss: 1.5324\n",
      "Epoch 120/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1726 - val_loss: 1.4052\n",
      "Epoch 121/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1644 - val_loss: 1.4199\n",
      "Epoch 122/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1714 - val_loss: 1.4247\n",
      "Epoch 123/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1699 - val_loss: 1.4053\n",
      "Epoch 124/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1709 - val_loss: 1.3953\n",
      "Epoch 125/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1668 - val_loss: 1.4763\n",
      "Epoch 126/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1711 - val_loss: 1.4102\n",
      "Epoch 127/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1525 - val_loss: 1.5558\n",
      "Epoch 128/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1632 - val_loss: 1.4620\n",
      "Epoch 129/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1572 - val_loss: 1.5142\n",
      "Epoch 130/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1620 - val_loss: 1.4658\n",
      "Epoch 131/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1555 - val_loss: 1.4090\n",
      "Epoch 132/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1483 - val_loss: 1.3752\n",
      "Epoch 133/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1456 - val_loss: 1.4021\n",
      "Epoch 134/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1553 - val_loss: 1.3807\n",
      "Epoch 135/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1492 - val_loss: 1.5061\n",
      "Epoch 136/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1542 - val_loss: 1.4125\n",
      "Epoch 137/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1445 - val_loss: 1.3787\n",
      "Epoch 138/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1514 - val_loss: 1.4363\n",
      "Epoch 139/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1461 - val_loss: 1.5140\n",
      "Epoch 140/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1359 - val_loss: 1.4212\n",
      "Epoch 141/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1458 - val_loss: 1.4311\n",
      "Epoch 142/200\n",
      "1535/1535 [==============================] - 4s 3ms/step - loss: 1.1313 - val_loss: 1.4355\n",
      "Epoch 143/200\n",
      "1535/1535 [==============================] - 4s 2ms/step - loss: 1.1562 - val_loss: 1.4234\n",
      "Epoch 144/200\n",
      "1535/1535 [==============================] - 4s 2ms/step - loss: 1.1385 - val_loss: 1.4654\n",
      "Epoch 145/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1302 - val_loss: 1.3952\n",
      "Epoch 146/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1446 - val_loss: 1.4305\n",
      "Epoch 147/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1336 - val_loss: 1.4041\n",
      "Epoch 148/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1373 - val_loss: 1.4195\n",
      "Epoch 149/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1311 - val_loss: 1.3808\n",
      "Epoch 150/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1290 - val_loss: 1.4084\n",
      "Epoch 151/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1338 - val_loss: 1.4312\n",
      "Epoch 152/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1251 - val_loss: 1.4420\n",
      "Epoch 153/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1241 - val_loss: 1.3800\n",
      "Epoch 154/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1305 - val_loss: 1.4012\n",
      "Epoch 155/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1227 - val_loss: 1.4488\n",
      "Epoch 156/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1236 - val_loss: 1.4638\n",
      "Epoch 157/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1168 - val_loss: 1.4122\n",
      "Epoch 158/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1213 - val_loss: 1.4322\n",
      "Epoch 159/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1231 - val_loss: 1.4226\n",
      "Epoch 160/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1176 - val_loss: 1.4179\n",
      "Epoch 161/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1150 - val_loss: 1.6346\n",
      "Epoch 162/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1120 - val_loss: 1.4077\n",
      "Epoch 163/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1150 - val_loss: 1.4251\n",
      "Epoch 164/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1301 - val_loss: 1.4418\n",
      "Epoch 165/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1108 - val_loss: 1.3912\n",
      "Epoch 166/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1106 - val_loss: 1.4580\n",
      "Epoch 167/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1179 - val_loss: 1.5765\n",
      "Epoch 168/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1148 - val_loss: 1.4038\n",
      "Epoch 169/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0954 - val_loss: 1.3918\n",
      "Epoch 170/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1063 - val_loss: 1.3811\n",
      "Epoch 171/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1005 - val_loss: 1.3833\n",
      "Epoch 172/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1096 - val_loss: 1.4109\n",
      "Epoch 173/200\n",
      "1535/1535 [==============================] - 2s 1ms/step - loss: 1.0967 - val_loss: 1.4234\n",
      "Epoch 174/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1079 - val_loss: 1.4493\n",
      "Epoch 175/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0966 - val_loss: 1.3909\n",
      "Epoch 176/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0947 - val_loss: 1.3955\n",
      "Epoch 177/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.1023 - val_loss: 1.4158\n",
      "Epoch 178/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0969 - val_loss: 1.3809\n",
      "Epoch 179/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0964 - val_loss: 1.4172\n",
      "Epoch 180/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0948 - val_loss: 1.4131\n",
      "Epoch 181/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0884 - val_loss: 1.4266\n",
      "Epoch 182/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0974 - val_loss: 1.4824\n",
      "Epoch 183/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0886 - val_loss: 1.4205\n",
      "Epoch 184/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0846 - val_loss: 1.3879\n",
      "Epoch 185/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0941 - val_loss: 1.4094\n",
      "Epoch 186/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0829 - val_loss: 1.4183\n",
      "Epoch 187/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.1002 - val_loss: 1.3888\n",
      "Epoch 188/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0851 - val_loss: 1.3951\n",
      "Epoch 189/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0865 - val_loss: 1.4100\n",
      "Epoch 190/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0804 - val_loss: 1.4946\n",
      "Epoch 191/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0869 - val_loss: 1.4121\n",
      "Epoch 192/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0782 - val_loss: 1.4163\n",
      "Epoch 193/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0765 - val_loss: 1.4356\n",
      "Epoch 194/200\n",
      "1535/1535 [==============================] - 2s 1ms/step - loss: 1.0822 - val_loss: 1.3934\n",
      "Epoch 195/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0862 - val_loss: 1.5255\n",
      "Epoch 196/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0782 - val_loss: 1.3544\n",
      "Epoch 197/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0748 - val_loss: 1.3769\n",
      "Epoch 198/200\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0711 - val_loss: 1.4079\n",
      "Epoch 199/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0750 - val_loss: 1.4317\n",
      "Epoch 200/200\n",
      "1535/1535 [==============================] - 2s 2ms/step - loss: 1.0841 - val_loss: 1.4351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1a82b6850>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Split the data into training, validation and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_data=(X_valid_scaled, y_valid), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0691c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test_scaled)\n",
    "\n",
    "results = [abs(i-j) for i,j in zip(y_predict, y_test)]\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3bc054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9146751\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f0553c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 1s 1ms/step - loss: 1.4577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4577219486236572"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f732d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1535/1535 [==============================] - 4s 3ms/step - loss: 1.0728 - val_loss: 1.4093\n",
      "Epoch 2/5\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0695 - val_loss: 1.4302\n",
      "Epoch 3/5\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0743 - val_loss: 1.4249\n",
      "Epoch 4/5\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0830 - val_loss: 1.4249\n",
      "Epoch 5/5\n",
      "1535/1535 [==============================] - 3s 2ms/step - loss: 1.0731 - val_loss: 1.4460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1899ce850>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=X_train_scaled, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(X_test_scaled, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42477530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "767d0821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 24756), started 17:46:43 ago. (Use '!kill 24756' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dea1fa10fb104584\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dea1fa10fb104584\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360663f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
